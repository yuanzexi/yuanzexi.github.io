# 【学习笔记】CUDA编程

> 版权声明：欢迎各位转载，但是未经作者本人同意，转载文章之后必须在文章页面明显位置给出作者和原文连接，否则保留追究法律责任的权利。 {{site.url}}{{page.url}}

## 什么是CUDA编程

近年来，计算机业界正在迅速迈进并行计算时代.在20 10年， 几乎所有的客户计算机都采用了多核处理器.从人门级的双核低端上网本，到8核和16核的工作站计算机，并行计算已不再是超级计算机或者大型机的专属技术。此外，一些电子设备，例如手机和便携式音乐播放器等，都开始集成并行计算功能，以提供比早期产品直强大的功能。随着时间的推移，软件开发人员将看到越来越多的并行计算平台和技术并需要基于这些平台和技术为不断成熟的用户群提供崭新且丰富的体验。

### CPU与GPU

衡量一个高性能处理器的时候，采用两个指标。

* 执行时间（Latency）：执行一项任务所花时间，采用时间单位。
* 吞吐量（Throughput）：单位时间完成的任务量。

而非常遗憾的是，这两项指标并不总是一致的，它们通常是矛盾的。比如说：
A地和B地相距4500 KM，从A到B可以有两种选择。一种方法是开跑车，车上乘坐2个人，以200 KM/H的速度开到B地；另一种方法是乘坐客车，车上乘坐着40个人，以50 KM/H的速度开到B地。

|方案|	Latency(hour)	|Throughput(people/hour)|
|---|---|---|
|开跑车|	4500 / 200 = 22.5	|2 / 22.5 = 0.0889|
|客车|	4500 / 50 = 90	|40 / 90 = 0.444|
虽然这个例子并不是很合理，但是它展示了 Latency 和 Throughput 的计算方式。
传统的 CPU 设计就是尝试去最优化执行时间，使其在每一项任务上的处理时间都能够达到最优。而 GPU 的设计与 CPU 不同，它的目标最大化吞吐量。因为在计算机图形学中，我们更加关心每秒能处理的像素数量，而不是每个像素需要花多少时间处理，甚至只要每秒能处理的像素数量只要能增加，即便单个像素处理的时间需要增加两倍也是可以接受的。

### GPU设计原则

- GPU 有许多简单的计算单元，它们组合在一块可以执行大量的计算，GPU 通常会牺牲更多的控制力来换取更强大的计算能力。
- GPU 采用显式并行编程模型，即编程的时候就是按多处理器的思路进行，而不是假设只有单个处理器，然后将程序交给编译器映射到多个处理上。
- GPU 设计的优化目标是 Throughput 而不是 Latency ，它可以接受单个任务执行时间延长，只要每秒能处理的任务总数能增加，也因此 GPU 适用于以 Throughput 为最重要衡量指标的应用程序中。

### CUDA架构

直到在GeForce 3系列发布五年之后，GPU计算才开始逐渐成为主流技术。在2006年11月，NVIDIA公布了业界的第一个DirectX 10 GPU，即GeForce 8800 GTX。GeForce 8800 GTX也是第一个基于NVIDIA的CUDA架构构建的GPU。CUDA架构专门为GPU计算设计了一种全新的模块，目的是减轻早起GPU计算中存在的一些限制，而正是这些限制使得之前的GPU在通用计算中没有得到广泛应用。
在之前的图形处理架构中，计算资源划分为顶点着色器和像素着色器，而CUOA架构则不同，它包含了一个统一的着色器流水线，使得执行通用计算的程序能够对芯片上的每个数学逻辑单元(Arithmetic Logic Unit, ALU) 进行排列。由于NVIDIA希望使新的图形处理器能适用
于通用计算，因此在实现这些ALU时都确保它们满足lEEE单精度浮点数学运算的需求，并且可以使用一个裁剪后的指令集来执行通用计算，而不是仅限于执行图形计算。此外， GPU上的执行单元不仅能任意地读/写内存，同时还能访问由软件管理的缓存， 也称为共享内存. CUOA架胡的所有这些功能部是为了使GPU不仅能执行传统的图形计算.还能高效地执行通用计算。

## 为什么用CUDA编程

自从CUDA C在2007年首次出现以来，许多企业都尝试以CUDA C为基础来构建应用程序，并获得了极大的成功。基于CUDA C编写的代码比之前的代码在性能上提升了多个数量级。而且，与传统在CPU上运行的应用程序相比，在NVIDIA图形处理器上运行的应用程序的单位成本和单位能耗都要低很多。目前CUDA C以及CUDA架构已经在以下领域获得了一些成功应用：

1. 医学图像
2. 计算流体动力学
3. 环境科学

## CUDA编程简介

### 开发环境

- 支持CUDA的图形处理器（GPU）
- [NVIDIA设备驱动程序](https://www.nvidia.com/Download/index.aspx?lang=en-us)
- [CUDA开发工具箱](https://developer.nvidia.com/accelerated-computing-toolkit)
    - WINDOWS 需要添加环境变量
    ```c
    CUDA_PATH = C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0
    ```
    - Visual Studio需要在项目设置中添加CUDA路径
    在“项目-> project_name 属性 -> VC++ 目录”中添加：
        - 可执行文件目录： `$(CUDA_PATH)\bin`
        - 包含目录：`$(CUDA_PATH)\include`
        - 库目录：`$(CUDA_PATH)\lib`
- 标准C编译器

### 一个简单的CUDA C程序

```c
#include <iostream>

/**
* 核函数定义
*/
__global__ void add(int a, int b, int *c) {
    *c = a + b;
}

int main(void){
    int c;
    int* dev_c;

    // GPU内存分配
    cudaMalloc((void**)&dev_c, sizeof(int));
    // 核函数调用
    add<<<1, 1,>>>(2, 7, dev_c);
    // GPU内存取值
    cudaMemcpy(&c, dev_c, sizeof(int),cudaMemcpyDeviceToHost);

    printf("2 + 7 = %d \n", c);
    printf("Hello, world!\n");

    // 释放GPU内存
    cudaFree(dev_c);
    return 0;
}
```

CUDA C与标准C语言程序在很大程度上是没什么区别的，我们将CPU以及系统内存成为**主机**，而将GPU及其内存称为**设备**，在GPU设备上执行的函数通常被称为**核函数（Kernel）**。

1. `__global__` 修饰符用来告诉编译器，该函数应该编译为在设备而不是主机上运行，即标记为“设备代码（Device Code）”，且表示该核函数既可以从主机端调用，也可以从设备端调用。另外，类似的修饰符还有`__device__`，表示只能从设备端调用。
2. `func_call<<<N, T>>>()` 核函数调用，尖括号定义调用核函数的线程与内存使用模式：
    - `N` -- 线程块数量（Block）
    - `T` -- 线程数（Thread）
    - 表示使用 `N * T` 个并行线程来执行该核函数
    - 核函数调用后，控制权立即返回给主机端执行后续代码，可用 `cudaDeviceSynchronize()`来强制主机端程度等待所有的核函数执行结束。
3. `cudaMalloc()`用来分配GPU内存, 该函数行为类似于C语言的`malloc()`, 但有其特殊的地方:
    - 可将`cudaMalloc()`分配的指针传递给设备/主机上执行的函数。  
    - 只能在设备代码（不能在主机代码）中使用`cudaMalloc()`分配的指针进行内存读写操作。
    - 须使用`cudaFree`来释放`cudaMalloc()`分配的内存。
4. `cudaMemcpy()`提供了除指针外的另一种访问设备上内存的方式，其行为类似于C语言中的`memcpy()`。该方法的最后一个参数决定该内存拷贝行为的方向：
    - `cudaMemcpyDeviceToHost`   设备 --> 主机
    - `cudaMemcpyHostToDevice`   主机 --> 设备
    - `cudaMemcpy()`是隐式同步的，即主机端程序必须等待数据拷贝完成后才能继续执行程序。

### CUDA并行编程

`kernel<<<N,T>>>`可以设置并行执行模式：

```c
__global__ void add(int *a, int *b, int *c)
{
    // 获取索引
    int tid = threadIdx.x + blockIdx.x * blockDim.x;
    while (tid < N){
        c[tid] = a[tid] + b[tid];
        // 递增索引
        tid += blockDim.x * gridDim.x;
    }
}
```

1. 内核启动所产生的所有线程统称为一个网格（Grid），同一网格中所有线程共享相同的全局内存空间。
    - Grid 包含多个 Block
    - Block 包含多个 Thread
    - 同一个 Block 内的 线程可以通过以下方式实现协作：
        - 同步
        - 共享内存
    - 不同 Block 内的线程不能协作
2. `N` -- 线程块数量（Block），我们将每个并行执行环境都称为一个线程块（Block），运行时，GPU将会为每个线程块创建一个核函数副本来执行；N个并行线程块组成的集合也称为一个线程格（Grid）。
    - `blockIdx`：内置变量，可以获取当前执行设备代码的线程块的索引，主要用于在核函数中获取线程块的索引。如`int tid = blockIdx.x;`。
    
    - 线程块数量限制：不超过65535
3. `T` -- 线程数（Thread），表示CUDA运行时在每个线程块中创建的线程数量，CUDA运行时总共启动的线程数量为`N * T`。
    - 最大线程数量不能超过设备属性结构中的`maxTreadsPerBlock`域的值，一般每个线程块不允许超过512个线程
    - 索引方式：`int tid = threadIdx.x + blockIdx.x * blockDim.x;`
        - `blockDim`：三维内置变量，线程块中每一维的线程数量。

4. 计算二维矩阵时，可将N，T也设置为二维来方便与矩阵进行对应计算。
    - `N = grid(DIM,DIM)`：二维线程格
        - 核函数获取二维线程格索引的方式
        ```
        int x = blockIdx.x;
        int y = blockIdx.y;
        int offset = x + y * gridDim.x;
        ```
        - `gridDim`：二维内置变量，用来保存线程格每一维的大小，核函数通过此内置变量来获取线程格大小。
        - 实际上，`grid(DIM,DIM)`的返回值类型是`dim3`，只不过最后一维的大小为1，这是CUDA为以后支持三维线程格做准备。但`gridDim`内置变量是2维的。
    - 二维图像计算案例
    ```c
    __global__ void kernel(unsigned char *ptr, int ticks){
        int x = threadIdx.x + blockIdx.x * blockDim.x;
        int y = threadIdx.y + blockIdx.y * blockDim.y;
        int offset = x + y * blockDim.x * gridDim.x;
        
        float fx = x - DIM/2;
        float fy = y - DIM/2;
        float d = sqrtf(fx * fx + fy * fy);
        unsigned char grey = (unsigned char) (128.0f + 127.0f) * cos(d/10.0f - ticks/7.0f) / (d/10.0f + 1.0f));
        ptr[offset * 4 + 0] = grey;
        ptr[offset * 4 + 1] = grey;
        ptr[offset * 4 + 2] = grey;
        ptr[offset * 4 + 3] = 255;
    }

    void generate_frame(DataBlock *d, int ticks){
        dim3 block3(DIM/16, DIM/16);
        dim3 threads(16,16);
        kernel<<<blocks, threads>>>(d->dev_bitmap, ticks);
    }
    ```

    - 这里使用了二维索引，blocks表示在线程格中包含的并行线程块数量，threads表示在每个线程块中包含的线程数量。这样可以使得每个线程都有一个唯一的索引(x, y)来与图像中的像素一一对应。

### 共享内存与同步

CUDA C编译器会为每个线程块创建该变量的一个副本，线程块中的每个线程都共享这块内存，但线程却无法看到也不能修改其他线程块的变量副本。这种机制能使得一个线程块中的多个线程能够在计算上进行通信和协作。而且，共享内存缓冲区驻留在物理GPU上，而不是在GPU之外的系统内存中。因此，在访问共享内存时的延迟要远远低于访问普通缓冲区的延迟，使得共享内存像每个线程块的高速缓存或者中间结果暂存器那样高效。然而，线程间通信需要同步机制。

```c
/**
*   向量内积运算
*/
__global__ void dot_product(float* a, float* b, float* c)
{
	int tid = threadIdx.x + blockIdx.x * blockDim.x;
	int cache_idx = threadIdx.x;

	__shared__ float cache[256];

	float sum = 0.0f;
	while (tid < N)
	{
		sum += a[tid] * b[tid];
		tid += blockDim.x * gridDim.x;
	}

	cache[cache_idx] = sum;
	__syncthreads();

	int i = blockDim.x / 2;
	while (i > 0)
	{
		if (cache_idx < i)
		{
			cache[cache_idx] += cache[cache_idx + i];
		}

		__syncthreads();
		i /= 2;
	}

	if (cache_idx == 0)
	{
		c[blockIdx.x] = cache[cache_idx];
	}
}

void gpu_thread_demo()
{
	int threadsPerBlock = 256;
	int blocksPerGrid = std::min(64, (N + threadsPerBlock - 1)/threadsPerBlock);
	std::vector<float> a(N, 2.0f);
	std::vector<float> b(N, 4.0f);
	std::vector<float> c(blocksPerGrid);
	float *dev_a, *dev_b, *dev_c;
	cudaError_t status = cudaError::cudaSuccess;
	status = cudaMalloc((void**)&dev_a, sizeof(float) * N);
	status = cudaMalloc((void**)&dev_b, sizeof(float) * N);
	status = cudaMalloc((void**)&dev_c, sizeof(float) * blocksPerGrid);

	status = cudaMemcpy(dev_a, a.data(), sizeof(float) * N, cudaMemcpyHostToDevice);
	status = cudaMemcpy(dev_b, b.data(), sizeof(float) * N, cudaMemcpyHostToDevice);

	dot_product<<<blocksPerGrid, threadsPerBlock>>>(dev_a, dev_b, dev_c);

	status = cudaMemcpy(c.data(), dev_c, sizeof(float)*blocksPerGrid, cudaMemcpyDeviceToHost);

	float total = 0.0f;
	for (auto num : c)
	{
		total += num;
	}

	cudaFree(dev_a);
	cudaFree(dev_b);
	cudaFree(dev_c);

	std::cout << "result = " << total << std::endl;
}
```

1. `__shared__` 用来声明一个共享内存缓冲区，该共享内存缓冲区由同一线程块中的所有 线程共享。可通过索引来隔离共享内存缓冲区的读写，但需确保所有对共享内存的写入操作在读取操作之前完成，而这需要`__syncthreads()`方法来确保。
2. `__syncthreads()` 对线程块中的线程进行同步。这个函数调用将确保线程块中的每个线程都执行完`__syncthreads()`之前的语句之后，才会执行下一条语句。然而，这样的机制会带来运行效率的降低，因为等待所有线程需要时间。
3. `__syncthreads()`不能放在if条件语句的scope之内，不然会因线程发散（Thread Divergence）问题引发未定义后果。当某些线程需要执行一条指令，而其他线程不需要执行时，这种情况就称为线程发散。在正常的环境中，发散的分支只会使得某些线程处于空闲状态，而其他线程将执行分支中的代码。但是，如果`__syncthreads()`位于发散分支中，那么一些线程将永远无法执行它，这与其确保每个线程都执行它才能执行下一条语句的原则相违背，于是GPU将挂起。

### 常量内存

由于在GPU上包含有数百个数学计算单元，因此性能瓶颈通常并不在于芯片的数学计算吞吐量，而是在于芯片的内存带宽。由于在图形处理器上包含了非常多的数学逻辑单元（ALU），因此有时输入数据的速率甚至无法维持如此高的计算速率。因此，有必要研究一些手段来减少计算问题时的内存通信量。CUDA C支持另一种类型的内存，即**常量内存**，用于**保存在核函数执行期间不会发生变化的数据**。NVIDIA硬件提供了64KB的常量内存，并且对于常量内存采取了不同于标准全局内存的处理方式。在某些情况下，用常量内存来替换全局内存能有效减少内存带宽。常量内存与全局内存的使用方式主要有以下几个不同：  

1. 声明方式不同，声明常量内存指针需要加`__constant__`修饰符。
2. 资源管理方式不同，当我们将其声明为常量内存时，我们需要为其提供一个固定的大小用于在编译期间静态地分配空间，无需再调用cudaMalloc()和cudaFree()来管理资源。
3. 内存操作方法不同，当需要将主机上的内容拷贝到常量内存时，须使用`cudaMemcpyToSymbol()`。与参数为`cudaMemcpyHostToDevice()`的`cudaMemcpy()`之间的唯一差异在于，`cudaMemcpyToSymbol()`会复制到常量内存，而`cudaMemcpy()`会复制到全局内存。

与从全局内存读取数据相比，从常量内存读取相同的数据可以节约内存带宽，主要有两个原因：

1. 对常量内存的单次读操作可以广播到其他的“邻近(Nearby)”线程，这将节约15次读取操作。
    - 线程束(Warp)：在CUDA架构中，线程束是指一个包含32个线程的集合，这个线程集合被“编织在一起”并且以“步调一致(Lockstep)”的形式执行。在程序中的每一行，线程束中的每个线程都将在不同的数据上执行相同的指令。
    - 当处理常量内存时，NVIDIA硬件将把单次内存读取操作广播到每个半线程束(Half-Warp)。如果在半线程束中的每个线程都从常量内存的相同地址上读取数据，那么GPU只会产生一次读取请求病在随后将数据广播到每个线程。如果从常量内存中读取大量数据，那么这种方式产生的内存流量只是使用全局内存时的1/16。
    - 半线程束广播功能实际上是一把双刃剑，虽然当所有16个线程都读取相同地址时，这个功能可以极大的提升性能，但当所有16个线程分班读取不同的地址时，它实际上会降低性能。如果半线程束中的所有16个线程需要访问常量内存中不同的数据时，那么这个16次不同的读取操作会被串行化，从而需要16倍的时间来发出请求。但如果从全局内存中读取，这些请求会同时发出。这种情况下，从常量内存读取就慢于从全局内存中读取。
2. 常量内存的数据将缓存起来，因此对相同地址的连续读操作将不会产生额外的内存通信量。

### GPU性能测量

当需要衡量算法在GPU上的性能时，用CPU或操作系统的某个定时器来测量可能会带来各种延迟（包括操作系统线程调度，高精度CPU计时器可用性等方面）。因此，为了测量GPU在某个任务上花费的时间，我们最好使用CUDA的事件API。
CUDA中的事件本质上是一个GPU时间戳，而获得该时间戳只需要两个步骤，创建一个事件，然后再记录一个事件。我们可以使用如下代码来统计工作时间。

```c
// 创建start和stop事件
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);

// 记录start时间戳
cudaEventRecord(start, 0);

// 在GPU上执行一些工作

// 记录stop时间戳
cudaEventRecord(stop, 0);
// 事件同步
cudaEventSynchronize(stop);

// cudaEventEplasedTime() 可计算间隔时间
float elapsedTime;
cudaEventElapsedTime(&elapsedTime, start, stop);

// 使用完事件后，需要摧毁他们
cudaEventDestroy(start);
cudaEventDestroy(stop);
```

为了保证GPU在执行完了`cudaEventRecord(stop, 0)`之前所有语句才调用该语句来测量正确的时间，我们需要使用`cudaEventSynchronize()`来进行事件同步。

### 纹理内存

和常量内存一样，纹理内存(Texture Memory)是另一种类型的只读内存，同样缓存在芯片上，在特定的访问模式中，纹理内存同样能够提升性能并减少内存流量，减少内存的请求并提供更高效的内存带宽。纹理内存是专为那些在内存访问模式中存在大量空间局部性(Spatial Locality)的图形应用程序而设计的，在某个计算应用程序中，这意味着一个线程读取的位置可能与邻近线程读取的位置“非常接近”。

```c
texture<float,2> texConstSrc;
texture<float,2> texIn;
texture<float,2> texOut;
float* a, b, c;
int DIM = 1000;

cudaMalloc((void**) &a, sizeof(float) * DIM);
cudaMalloc((void**) &b, sizeof(float) * DIM);
cudaMalloc((void**) &c, sizeof(float) * DIM);

// 绑定纹理内存
cudaChannelFormatDesc desc = cudaCreateChannelDesc<float>();
cudaBindTexture2D(NULL, texConstSrc, a, desc, DIM, DIM, sizeof(float) * DIM);
cudaBindTexture2D(NULL, texIn, b, desc, DIM, DIM, sizeof(float) * DIM);
cudaBindTexture2D(NULL, texOut, c, desc, DIM, DIM, sizeof(float) * DIM);

// 读取纹理内存
__global__ void blend_kernel(float *dst, bool dstOut) {
    int x = threadIdx.x + blockIdx.x * blockDim.x;
    int x = threadIdx.y + blockIdx.y * blockDim.y;
    int offset = x + y * blockDim.x * gridDim.x;
    
    float t, l, c, r, b;
    t = tex2D(texIn, x, y-1);
    l = tex2D(texIn, x-1, y);
    c = tex2D(texIn, x, y);
    r = tex2D(texIn, x+1, y);
    b = tex2D(texIn, x, y+1);
    dst[offset] = c + SPEED * (t + b + r + l - 4 * c);
}

// 执行运算

// 取消纹理绑定并释放内存
cudaUnbindTexture(texIn);
cudaUnbindTexture(texOut);
cudaUnbindTexture(texConstSrc);
cudaFree(a);
cudaFree(b);
cudaFree(c);
```

1. `texture<float, 2> tex_name`，声明二维纹理引用：需声明为全局变量。一维纹理引用为`texture<float> tex_name`。
2. `cudaChannelFormatDesc desc = cudaCreateChannelDesc<float>()`,通道格式描述符(Channel Format Descriptor)声明，用于二维纹理绑定。
3. `cudaBindTexture2D(NULL, tex_name,  device_mem_ptr, channel_desc, DIM, DIM, size)`，二维纹理内存绑定：可将指定的全局内存`device_mem_ptr`绑定到对应的纹理引用`tex_name`，来将其作为纹理内存使用，纹理维数为(DIM, DIM)。一维纹理绑定为`cudaBindTexture(NULL, tex_name, device_mem_ptr, size)`
4. `tex2D(texIn, x, y)`，编译器内置函数，用于在核函数内进行纹理内存的读取。当使用tex2D()时，我们无需担心发生溢出问题，如果x或y小于0，那么text2D()将返回0处的值。同理，若某个值大于宽度，则返回位于宽度处的值。一维纹理内存读取为`tex1Dfetch(tex_name, index)`，使用时需注意溢出问题。
5. `cudaUnbindTexture(tex_name)`，取消纹理内存绑定，取消纹理绑定后，仍需释放对应的使用内存。
6. 一维纹理与二维纹理没有性能上的差异，但二维纹理的代码相对更整洁，且能自动处理边界问题。

## 常见问题

## 参考文献

1. [并行编程——Lesson 1：GPU 编程模型](https://www.jianshu.com/p/999b3a342247)